{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path):\n",
    "    with open(path, 'r') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_at_k(r, k):\n",
    "    r = np.asfarray(r)[:k]\n",
    "    if r.size:\n",
    "        return r[0] + np.sum(r[1:] / np.log2(np.arange(3, r.size + 2)))\n",
    "    return 0.\n",
    "\n",
    "\n",
    "# compute ndcg@k (dcg@k / idcg@k) for a single sample\n",
    "def get_ndcg(r, ref, k):\n",
    "    dcg_max = dcg_at_k(ref, k)\n",
    "    if not dcg_max:\n",
    "        return 0.\n",
    "    dcg = dcg_at_k(r, k)\n",
    "    return dcg / dcg_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read validation prediction from the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = read_json('prediction_result/valid_pred_model1.json')\n",
    "pred2 = read_json('prediction_result/valid_pred_model2.json')\n",
    "gt = read_json('./data/valid/valid_answer.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search for the best threshold `t` to ensemble the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best score: 0.7094, best t: 0.0000\n",
      "best score: 0.7101, best t: 0.0101\n",
      "best score: 0.7104, best t: 0.0202\n",
      "best score: 0.7115, best t: 0.0303\n",
      "best score: 0.7119, best t: 0.0404\n",
      "best score: 0.7129, best t: 0.0505\n",
      "best score: 0.7136, best t: 0.0606\n",
      "best score: 0.7137, best t: 0.0707\n",
      "best score: 0.7143, best t: 0.0909\n",
      "best score: 0.7151, best t: 0.1010\n",
      "best score: 0.7160, best t: 0.1111\n",
      "best score: 0.7171, best t: 0.1212\n",
      "best score: 0.7178, best t: 0.1313\n",
      "best score: 0.7187, best t: 0.1414\n",
      "best score: 0.7193, best t: 0.1515\n",
      "best score: 0.7199, best t: 0.1616\n",
      "best score: 0.7200, best t: 0.2020\n",
      "best score: 0.7208, best t: 0.2121\n",
      "best score: 0.7220, best t: 0.2222\n",
      "best score: 0.7233, best t: 0.2323\n",
      "best score: 0.7242, best t: 0.2424\n",
      "best score: 0.7261, best t: 0.2727\n",
      "best score: 0.7268, best t: 0.2828\n",
      "best score: 0.7273, best t: 0.2929\n",
      "best score: 0.7282, best t: 0.3030\n",
      "best score: 0.7291, best t: 0.3232\n",
      "best score: 0.7299, best t: 0.3535\n",
      "best score: 0.7308, best t: 0.3636\n",
      "best score: 0.7314, best t: 0.3737\n",
      "best score: 0.7322, best t: 0.3838\n",
      "best score: 0.7325, best t: 0.3939\n",
      "best score: 0.7335, best t: 0.4040\n",
      "best score: 0.7338, best t: 0.4242\n",
      "best score: 0.7345, best t: 0.4343\n",
      "best score: 0.7350, best t: 0.4444\n",
      "best score: 0.7354, best t: 0.4646\n",
      "best score: 0.7355, best t: 0.4848\n",
      "best score: 0.7359, best t: 0.4949\n",
      "best score: 0.7360, best t: 0.5051\n",
      "best score: 0.7370, best t: 0.5253\n",
      "best score: 0.7370, best t: 0.5455\n",
      "best score: 0.7377, best t: 0.5556\n",
      "best score: 0.7387, best t: 0.5657\n",
      "best score: 0.7399, best t: 0.5758\n",
      "best score: 0.7401, best t: 0.5859\n",
      "best score: 0.7409, best t: 0.6061\n",
      "best score: 0.7411, best t: 0.6162\n",
      "best score: 0.7417, best t: 0.6263\n"
     ]
    }
   ],
   "source": [
    "best_score = 0\n",
    "t = 0\n",
    "for i in np.linspace(0, 1, 100):\n",
    "    pred = {}\n",
    "    for k in pred1.keys():\n",
    "        v1 = pred1[k]\n",
    "        v2 = pred2[k]\n",
    "        v1 = dict(v1)\n",
    "        v2 = dict(v2)\n",
    "        v = []\n",
    "        for kk in v1.keys():\n",
    "            v.append((kk, i * v1[kk] + (1 - i) * v2[kk]))\n",
    "    #     v = [(v2[i][0], v1[i][1] + v2[i][1]) for i in range(len(v1))]\n",
    "        v = sorted(v, key=lambda x: x[1], reverse=True)\n",
    "        pred[k] = v\n",
    "        \n",
    "    score = 0\n",
    "    k = 5\n",
    "    for key, val in gt.items():\n",
    "        ground_truth_ids = [str(x) for x in val]\n",
    "        predictions = [x[0] for x in pred[key][:k]]\n",
    "        ref_vec = [1.0] * len(ground_truth_ids)\n",
    "\n",
    "        pred_vec = [1.0 if pid in ground_truth_ids else 0.0 for pid in predictions]\n",
    "        score += get_ndcg(pred_vec, ref_vec, k)\n",
    "        # print(key)\n",
    "        # print([pid for pid in predictions if pid not in ground_truth_ids])\n",
    "        # print('========')\n",
    "        # score += len(set(predictions).intersection(ground_truth_ids)) / len(ground_truth_ids)\n",
    "    score = score / len(gt)\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        t = i\n",
    "        print('best score: %.4f, best t: %.4f' % (score, i))\n",
    "#     print('ndcg@%d: %.4f' % (k, score / len(gt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read testing prediction from the two models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred1 = read_json('prediction_result/test_pred_model1.json')\n",
    "pred2 = read_json('prediction_result/test_pred_model2.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the ensembled score for the testing data and output submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = {}\n",
    "for k in pred1.keys():\n",
    "    v1 = pred1[k]\n",
    "    v2 = pred2[k]\n",
    "    v1 = dict(v1)\n",
    "    v2 = dict(v2)\n",
    "    v = []\n",
    "    for kk in v1.keys():\n",
    "        v.append((kk, t * v1[kk] + (1 - t) * v2[kk]))\n",
    "#     v = [(v2[i][0], v1[i][1] + v2[i][1]) for i in range(len(v1))]\n",
    "    v = sorted(v, key=lambda x: x[1], reverse=True)\n",
    "    pred[k] = v\n",
    "\n",
    "submission = []\n",
    "for k, v in pred.items():\n",
    "    v = sorted(v, key=lambda x: x[1], reverse=True)\n",
    "    v = [x[0] for x in v]\n",
    "    submission.append([k] + v[:5])\n",
    "\n",
    "submission = pd.DataFrame(submission, columns=['query-id', 'product1', 'product2',\n",
    "                                               'product3', 'product4', 'product5'])\n",
    "submission.to_csv('prediction_result/submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}